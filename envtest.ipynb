{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2025-01-17 16:03:06,434:jax._src.xla_bridge:969: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    }
   ],
   "source": [
    "from luxai_s3.wrappers import LuxAIS3GymEnv, RecordEpisode\n",
    "from typing import Dict\n",
    "from argparse import Namespace\n",
    "import jax\n",
    "from lux.utils import direction_to\n",
    "from luxai_s3.params import EnvParams\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import namedtuple, defaultdict\n",
    "\n",
    "# from lux.config import EnvConfig\n",
    "from lux.kit import from_json\n",
    "env = LuxAIS3GymEnv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## QMIX Neural Network\n",
    "## 매우 제한적인 모델, 수정 필요\n",
    "## 인풋으로 자기 자신의 observation만 받고, history를 받지도 않는다\n",
    "\n",
    "class QMixNet(nn.Module):\n",
    "    def __init__(self, num_agents, state_dim, action_dim=6, hidden_dim=32):\n",
    "        super(QMixNet, self).__init__()\n",
    "        self.num_agents = num_agents\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        # State-dependent weights for mixing\n",
    "        self.hyper_w1 = nn.LazyLinear(action_dim * hidden_dim)\n",
    "        self.hyper_w2 = nn.LazyLinear(hidden_dim)\n",
    "        self.hyper_w3 = nn.LazyLinear(1)\n",
    "        self.hyper_b1 = nn.LazyLinear(num_agents * hidden_dim)\n",
    "        self.hyper_b2 = nn.LazyLinear(num_agents)\n",
    "\n",
    "        self.elu = nn.ELU()\n",
    "\n",
    "    def forward(self, agent_qs, state_inputs):\n",
    "        batch_size = agent_qs.size(0)\n",
    "        \n",
    "        # Flatten state inputs for the hypernetworks\n",
    "        state_inputs = state_inputs.view(batch_size, -1)\n",
    "\n",
    "        # Compute weights and biases\n",
    "        w1 = self.hyper_w1(state_inputs).view(batch_size, self.action_dim, -1)\n",
    "        w2 = self.hyper_w2(state_inputs).view(batch_size, -1, 1)\n",
    "        b1 = self.hyper_b1(state_inputs).view(batch_size, self.num_agents, -1)\n",
    "        b2 = self.hyper_b2(state_inputs).view(batch_size, self.num_agents, 1)\n",
    "\n",
    "        # Mixing process\n",
    "        hidden = self.elu(torch.bmm(agent_qs, w1) + b1)\n",
    "        \n",
    "        q_total = torch.bmm(hidden, w2) + b2\n",
    "        q_total = self.hyper_w3(q_total.squeeze())\n",
    "\n",
    "        return q_total\n",
    "\n",
    "# Replay Buffer\n",
    "Transition = namedtuple(\"Transition\", (\"state\", \"actions\", \"rewards\", \"next_state\", \"dones\"))\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, *args):\n",
    "        if len(self.memory) >= self.capacity:\n",
    "            self.memory.pop(0)\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.memory), batch_size, replace=False)\n",
    "        batch = [self.memory[idx] for idx in indices]\n",
    "        return Transition(*zip(*batch))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "# 초기값 및 모델 하이퍼파라미터 세팅\n",
    "\n",
    "num_agents = 16\n",
    "np.random.seed(16)\n",
    "env = LuxAIS3GymEnv()\n",
    "env = RecordEpisode(env, save_dir=\"episodes\")\n",
    "env_params = EnvParams(map_type=1, max_steps_in_match=100)\n",
    "state_dim = 2\n",
    "action_dim = 6\n",
    "mixing_dim = 32\n",
    "N = env_params.max_steps_in_match * env_params.match_count_per_episode\n",
    "\n",
    "q_networks = [nn.Sequential(nn.Linear(state_dim, 64), nn.ReLU(), nn.Linear(64, action_dim)) for _ in range(num_agents)]\n",
    "optimizers = [optim.Adam(q.parameters(), lr=0.001) for q in q_networks]\n",
    "mixing_network = QMixNet(num_agents, state_dim, action_dim, mixing_dim)\n",
    "mixing_optimizer = optim.Adam(mixing_network.parameters(), lr=0.001)\n",
    "\n",
    "buffer = ReplayBuffer(capacity=3000)\n",
    "batch_size = 32\n",
    "epsilon = 0.08\n",
    "num_episodes = 100\n",
    "\n",
    "def select_action(q_values, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(q_values.size(-1))\n",
    "    return [torch.argmax(q_values).item(),0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Loss: 725.9942. my_team_wins: 4\n",
      "Episode 1, Loss: 666.0987. my_team_wins: 3\n",
      "Episode 2, Loss: 1397.9777. my_team_wins: 3\n",
      "Episode 3, Loss: 554.3622. my_team_wins: 1\n",
      "Episode 4, Loss: 333.6446. my_team_wins: 1\n",
      "Episode 5, Loss: 474.9640. my_team_wins: 3\n",
      "Episode 6, Loss: 925.1939. my_team_wins: 4\n",
      "Episode 7, Loss: 943.4852. my_team_wins: 2\n",
      "Episode 8, Loss: 801.5966. my_team_wins: 3\n",
      "Episode 9, Loss: 138.7711. my_team_wins: 1\n",
      "Episode 10, Loss: 304.3309. my_team_wins: 3\n",
      "Episode 11, Loss: 494.7083. my_team_wins: 2\n",
      "Episode 12, Loss: 221.8595. my_team_wins: 0\n",
      "Episode 13, Loss: 168.3093. my_team_wins: 4\n",
      "Episode 14, Loss: 197.1183. my_team_wins: 4\n",
      "Episode 15, Loss: 175.3127. my_team_wins: 4\n",
      "Episode 16, Loss: 365.1474. my_team_wins: 3\n",
      "Episode 17, Loss: 334.0553. my_team_wins: 3\n",
      "Episode 18, Loss: 241.1416. my_team_wins: 0\n",
      "Episode 19, Loss: 562.9294. my_team_wins: 3\n",
      "Episode 20, Loss: 193.5354. my_team_wins: 3\n",
      "Episode 21, Loss: 504.6407. my_team_wins: 2\n",
      "Episode 22, Loss: 432.3690. my_team_wins: 3\n",
      "Episode 23, Loss: 219.5867. my_team_wins: 2\n",
      "Episode 24, Loss: 338.3853. my_team_wins: 4\n",
      "Episode 25, Loss: 264.3065. my_team_wins: 4\n",
      "Episode 26, Loss: 791.7608. my_team_wins: 3\n",
      "Episode 27, Loss: 147.3098. my_team_wins: 2\n",
      "Episode 28, Loss: 215.4743. my_team_wins: 4\n",
      "Episode 29, Loss: 149.4909. my_team_wins: 2\n",
      "Episode 30, Loss: 69.8156. my_team_wins: 2\n",
      "Episode 31, Loss: 374.6786. my_team_wins: 1\n",
      "Episode 32, Loss: 333.4994. my_team_wins: 3\n",
      "Episode 33, Loss: 732.2024. my_team_wins: 4\n",
      "Episode 34, Loss: 671.5975. my_team_wins: 4\n",
      "Episode 35, Loss: 437.2359. my_team_wins: 3\n",
      "Episode 36, Loss: 208.6611. my_team_wins: 3\n",
      "Episode 37, Loss: 563.5693. my_team_wins: 2\n",
      "Episode 38, Loss: 144.1089. my_team_wins: 2\n",
      "Episode 39, Loss: 100.1999. my_team_wins: 1\n",
      "Episode 40, Loss: 124.0452. my_team_wins: 4\n",
      "Episode 41, Loss: 100.1290. my_team_wins: 2\n",
      "Episode 42, Loss: 106.9065. my_team_wins: 4\n",
      "Episode 43, Loss: 50.6533. my_team_wins: 5\n",
      "Episode 44, Loss: 72.3193. my_team_wins: 1\n",
      "Episode 45, Loss: 94.6554. my_team_wins: 3\n",
      "Episode 46, Loss: 132.5048. my_team_wins: 1\n",
      "Episode 47, Loss: 43.7498. my_team_wins: 3\n",
      "Episode 48, Loss: 30.3098. my_team_wins: 3\n",
      "Episode 49, Loss: 93.7994. my_team_wins: 3\n",
      "Episode 50, Loss: 77.5495. my_team_wins: 3\n",
      "Episode 51, Loss: 202.6466. my_team_wins: 4\n",
      "Episode 52, Loss: 274.8730. my_team_wins: 4\n",
      "Episode 53, Loss: 97.0642. my_team_wins: 5\n",
      "Episode 54, Loss: 274.5525. my_team_wins: 4\n",
      "Episode 55, Loss: 97.3571. my_team_wins: 1\n",
      "Episode 56, Loss: 93.7355. my_team_wins: 2\n",
      "Episode 57, Loss: 306.2148. my_team_wins: 2\n",
      "Episode 58, Loss: 191.1969. my_team_wins: 5\n",
      "Episode 59, Loss: 135.7496. my_team_wins: 3\n",
      "Episode 60, Loss: 110.2694. my_team_wins: 3\n",
      "Episode 61, Loss: 463.9617. my_team_wins: 3\n",
      "Episode 62, Loss: 233.9582. my_team_wins: 2\n",
      "Episode 63, Loss: 182.7294. my_team_wins: 3\n",
      "Episode 64, Loss: 89.7881. my_team_wins: 1\n",
      "Episode 65, Loss: 89.6744. my_team_wins: 5\n",
      "Episode 66, Loss: 160.3186. my_team_wins: 3\n",
      "Episode 67, Loss: 213.2014. my_team_wins: 3\n",
      "Episode 68, Loss: 106.9130. my_team_wins: 2\n",
      "Episode 69, Loss: 113.3535. my_team_wins: 3\n",
      "Episode 70, Loss: 59.4762. my_team_wins: 2\n",
      "Episode 71, Loss: 53.1043. my_team_wins: 2\n",
      "Episode 72, Loss: 99.2496. my_team_wins: 4\n",
      "Episode 73, Loss: 87.8581. my_team_wins: 5\n",
      "Episode 74, Loss: 47.7282. my_team_wins: 4\n",
      "Episode 75, Loss: 58.1467. my_team_wins: 2\n",
      "Episode 76, Loss: 56.7453. my_team_wins: 3\n",
      "Episode 77, Loss: 131.3957. my_team_wins: 2\n",
      "Episode 78, Loss: 53.9625. my_team_wins: 3\n",
      "Episode 79, Loss: 35.0767. my_team_wins: 3\n",
      "Episode 80, Loss: 114.6044. my_team_wins: 2\n",
      "Episode 81, Loss: 98.7727. my_team_wins: 3\n",
      "Episode 82, Loss: 77.7366. my_team_wins: 3\n",
      "Episode 83, Loss: 147.4833. my_team_wins: 4\n",
      "Episode 84, Loss: 137.2801. my_team_wins: 1\n",
      "Episode 85, Loss: 116.9712. my_team_wins: 1\n",
      "Episode 86, Loss: 249.2669. my_team_wins: 3\n",
      "Episode 87, Loss: 56.0217. my_team_wins: 2\n",
      "Episode 88, Loss: 96.2841. my_team_wins: 5\n",
      "Episode 89, Loss: 212.6611. my_team_wins: 4\n",
      "Episode 90, Loss: 326.3932. my_team_wins: 1\n",
      "Episode 91, Loss: 185.4444. my_team_wins: 5\n",
      "Episode 92, Loss: 72.3003. my_team_wins: 4\n",
      "Episode 93, Loss: 41.2857. my_team_wins: 3\n",
      "Episode 94, Loss: 39.2463. my_team_wins: 2\n",
      "Episode 95, Loss: 59.1092. my_team_wins: 3\n",
      "Episode 96, Loss: 195.0910. my_team_wins: 1\n",
      "Episode 97, Loss: 171.5409. my_team_wins: 3\n",
      "Episode 98, Loss: 66.7628. my_team_wins: 4\n",
      "Episode 99, Loss: 24.5179. my_team_wins: 2\n"
     ]
    }
   ],
   "source": [
    "# training process\n",
    "# 해당 셀을 실행하면 학습이 진행됩니다.\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    states, info = env.reset(seed=1, options=dict(params=env_params))\n",
    "    done = False\n",
    "    episode_transitions = []\n",
    "    i = 0\n",
    "\n",
    "    while not done or i < N:\n",
    "        # 각 플레이어의 행동, q value, observation을 담는 object의 지정\n",
    "        actions = np.zeros((num_agents, 3), dtype=int)\n",
    "        opp_actions = np.zeros((num_agents, 3), dtype=int)\n",
    "        q_values = {}\n",
    "        opp_q_values = {}\n",
    "        state = states['player_0'].units.position[0]\n",
    "        opp_state = states['player_1'].units.position[0]\n",
    "        \n",
    "        # 플레이어 0의 에이전트 별 q value 계산 및 decentralized action 생성\n",
    "        for agent_id, q_network in zip(range(num_agents), q_networks):\n",
    "            obs = torch.tensor(np.array(state[agent_id]), dtype=torch.float32).unsqueeze(0)\n",
    "            q_values[agent_id] = q_network(obs)\n",
    "            actions[agent_id] = select_action(q_values[agent_id], epsilon)\n",
    "\n",
    "        # self-play 기반 opponent action 생성\n",
    "        # 위 for loop와 동일\n",
    "        with torch.no_grad():\n",
    "                for agent_id, q_network in zip(range(num_agents), q_networks):\n",
    "                    obs_opp = torch.tensor(np.array(opp_state[agent_id]), dtype=torch.float32).unsqueeze(0)\n",
    "                    opp_q_values[agent_id] = q_network(obs_opp)\n",
    "                    opp_actions[agent_id] = select_action(opp_q_values[agent_id], epsilon)\n",
    "\n",
    "        #opp_actions = env.action_space.sample()['player_1'] # random action opponents로 학습도 가능\n",
    "\n",
    "        act = {'player_0': actions, 'player_1': opp_actions}\n",
    "\n",
    "        # 버퍼에는 내 플레이어의 정보들만 저장\n",
    "        next_state, rewards, terminated, truncated, info = env.step(act)\n",
    "        dones = terminated['player_0'] + truncated['player_0']\n",
    "        buffer.push(state, actions, rewards['player_0'], next_state['player_0'].units.position[0], dones)\n",
    "        state = next_state\n",
    "        done = dones\n",
    "        i += 1\n",
    "\n",
    "    # optimizing 하는 루프\n",
    "    if len(buffer) >= batch_size:\n",
    "        batch = buffer.sample(batch_size)\n",
    "        state_batch = torch.tensor(np.array([s for s in batch.state]), dtype=torch.float32)\n",
    "        action_batch = torch.tensor(np.array([a for a in batch.actions]), dtype=torch.int64)\n",
    "        reward_batch = torch.tensor(np.array([r for r in batch.rewards]), dtype=torch.float32)\n",
    "        next_state_batch = torch.tensor(np.array([ns for ns in batch.next_state]), dtype=torch.float32)\n",
    "        done_batch = torch.tensor(np.array([d for d in batch.dones]), dtype=torch.float32)\n",
    "\n",
    "        # Calculate individual Q-values\n",
    "        agent_qs = []\n",
    "        for i, q_network in enumerate(q_networks):\n",
    "            agent_qs.append(q_network(state_batch[:, i, :]))\n",
    "        agent_qs = torch.stack(agent_qs, dim=1)\n",
    "\n",
    "        # Calculate total Q-value using mixing network\n",
    "        state_inputs = state_batch.view(batch_size, -1)\n",
    "        q_total = mixing_network(agent_qs, state_inputs)\n",
    "\n",
    "        # Calculate individual next Q-values\n",
    "        agent_qs_next = []\n",
    "        for i, q_network in enumerate(q_networks):\n",
    "            agent_qs_next.append(q_network(next_state_batch[:, i, :]))\n",
    "        agent_qs_next = torch.stack(agent_qs_next, dim=1)\n",
    "\n",
    "        # Calculate total next Q-value using mixing network\n",
    "        next_state_inputs = next_state_batch.view(batch_size, -1)\n",
    "        with torch.no_grad():\n",
    "            q_total_next = mixing_network(agent_qs_next, next_state_inputs)\n",
    "\n",
    "        # Compute loss and update networks\n",
    "        loss = torch.mean((reward_batch + q_total_next - q_total) ** 2)\n",
    "\n",
    "        for optimizer in optimizers:\n",
    "            optimizer.zero_grad()\n",
    "        mixing_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        for optimizer in optimizers:\n",
    "            optimizer.step()\n",
    "        mixing_optimizer.step()\n",
    "\n",
    "    if episode % 1 == 0:\n",
    "        print(f\"Episode {episode}, Loss: {loss.item():.4f}. my_team_wins: {rewards['player_0']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수동 저장\n",
    "# 체크포인트 생성 및 모델의 자동 업데이트 기능은 아직 없습니다\n",
    "q_net = nn.ModuleList(q_networks)\n",
    "torch.save(q_net, \"q_net.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "24test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
