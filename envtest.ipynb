{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2025-01-08 15:39:58,844:jax._src.xla_bridge:969: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from luxai_s3.wrappers import LuxAIS3GymEnv, RecordEpisode\n",
    "import flax.serialization\n",
    "from typing import Dict\n",
    "import sys\n",
    "from argparse import Namespace\n",
    "import jax\n",
    "import numpy as np\n",
    "from lux.utils import direction_to\n",
    "from luxai_s3.params import EnvParams\n",
    "\n",
    "\n",
    "# from lux.config import EnvConfig\n",
    "from lux.kit import from_json\n",
    "env = LuxAIS3GymEnv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, player: str, env_cfg) -> None:\n",
    "        self.player = player\n",
    "        self.opp_player = \"player_1\" if self.player == \"player_0\" else \"player_0\"\n",
    "        self.team_id = 0 if self.player == \"player_0\" else 1\n",
    "        self.opp_team_id = 1 if self.team_id == 0 else 0\n",
    "        np.random.seed(0)\n",
    "        self.env_cfg = env_cfg\n",
    "        \n",
    "        self.relic_node_positions = []\n",
    "        self.discovered_relic_nodes_ids = set()\n",
    "        self.unit_explore_locations = dict()\n",
    "\n",
    "    def act(self, step: int, obs, remainingOverageTime: int = 60):\n",
    "        \"\"\"implement this function to decide what actions to send to each available unit. \n",
    "        \n",
    "        step is the current timestep number of the game starting from 0 going up to max_steps_in_match * match_count_per_episode - 1.\n",
    "        \"\"\"\n",
    "        unit_mask = np.array(obs[\"units_mask\"][self.team_id]) # shape (max_units, )\n",
    "        unit_positions = np.array(obs[\"units\"][\"position\"][self.team_id]) # shape (max_units, 2)\n",
    "        unit_energys = np.array(obs[\"units\"][\"energy\"][self.team_id]) # shape (max_units, 1)\n",
    "        observed_relic_node_positions = np.array(obs[\"relic_nodes\"]) # shape (max_relic_nodes, 2)\n",
    "        observed_relic_nodes_mask = np.array(obs[\"relic_nodes_mask\"]) # shape (max_relic_nodes, )\n",
    "        team_points = np.array(obs[\"team_points\"]) # points of each team, team_points[self.team_id] is the points of the your team\n",
    "        \n",
    "        # ids of units you can control at this timestep\n",
    "        available_unit_ids = np.where(unit_mask)[0]\n",
    "        # visible relic nodes\n",
    "        visible_relic_node_ids = set(np.where(observed_relic_nodes_mask)[0])\n",
    "        \n",
    "        actions = np.zeros((self.env_cfg[\"max_units\"], 3), dtype=int)\n",
    "\n",
    "\n",
    "        # basic strategy here is simply to have some units randomly explore and some units collecting as much energy as possible\n",
    "        # and once a relic node is found, we send all units to move randomly around the first relic node to gain points\n",
    "        # and information about where relic nodes are found are saved for the next match\n",
    "        \n",
    "        # save any new relic nodes that we discover for the rest of the game.\n",
    "        for id in visible_relic_node_ids:\n",
    "            if id not in self.discovered_relic_nodes_ids:\n",
    "                self.discovered_relic_nodes_ids.add(id)\n",
    "                self.relic_node_positions.append(observed_relic_node_positions[id])\n",
    "            \n",
    "\n",
    "        # unit ids range from 0 to max_units - 1\n",
    "        for unit_id in available_unit_ids:\n",
    "            unit_pos = unit_positions[unit_id]\n",
    "            unit_energy = unit_energys[unit_id]\n",
    "            if len(self.relic_node_positions) > 0:\n",
    "                nearest_relic_node_position = self.relic_node_positions[0]\n",
    "                manhattan_distance = abs(unit_pos[0] - nearest_relic_node_position[0]) + abs(unit_pos[1] - nearest_relic_node_position[1])\n",
    "                \n",
    "                # if close to the relic node we want to hover around it and hope to gain points\n",
    "                if manhattan_distance <= 4:\n",
    "                    random_direction = np.random.randint(0, 5)\n",
    "                    actions[unit_id] = [random_direction, 0, 0]\n",
    "                else:\n",
    "                    # otherwise we want to move towards the relic node\n",
    "                    actions[unit_id] = [direction_to(unit_pos, nearest_relic_node_position), 0, 0]\n",
    "            else:\n",
    "                # randomly explore by picking a random location on the map and moving there for about 20 steps\n",
    "                if step % 20 == 0 or unit_id not in self.unit_explore_locations:\n",
    "                    rand_loc = (np.random.randint(0, self.env_cfg[\"map_width\"]), np.random.randint(0, self.env_cfg[\"map_height\"]))\n",
    "                    self.unit_explore_locations[unit_id] = rand_loc\n",
    "                actions[unit_id] = [direction_to(unit_pos, self.unit_explore_locations[unit_id]), 0, 0]\n",
    "        return actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training setup\n",
    "np.random.seed(2)\n",
    "env = LuxAIS3GymEnv()\n",
    "env = RecordEpisode(env, save_dir=\"episodes\")\n",
    "env_params = EnvParams(map_type=1, max_steps_in_match=100)\n",
    "obs, info = env.reset(seed=1, options=dict(params=env_params))\n",
    "\n",
    "N = env_params.max_steps_in_match * env_params.match_count_per_episode\n",
    "for _ in range(N):\n",
    "    obs, reward, terminated, truncated, info=  env.step(env.action_space.sample())\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from gym.spaces import Box, Discrete\n",
    "from collections import namedtuple, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## QMIX Neural Network\n",
    "class QMixNet(nn.Module):\n",
    "    def __init__(self, num_agents, state_dim, action_dim=6, hidden_dim=32):\n",
    "        super(QMixNet, self).__init__()\n",
    "        self.num_agents = num_agents\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        # State-dependent weights for mixing\n",
    "        self.hyper_w1 = nn.LazyLinear(action_dim * hidden_dim)\n",
    "        self.hyper_w2 = nn.LazyLinear(hidden_dim)\n",
    "        self.hyper_w3 = nn.LazyLinear(1)\n",
    "        self.hyper_b1 = nn.LazyLinear(num_agents * hidden_dim)\n",
    "        self.hyper_b2 = nn.LazyLinear(num_agents)\n",
    "\n",
    "        self.elu = nn.ELU()\n",
    "\n",
    "    def forward(self, agent_qs, state_inputs):\n",
    "        batch_size = agent_qs.size(0)\n",
    "        \n",
    "        # Flatten state inputs for the hypernetworks\n",
    "        state_inputs = state_inputs.view(batch_size, -1)\n",
    "\n",
    "        # Compute weights and biases\n",
    "        w1 = self.hyper_w1(state_inputs).view(batch_size, self.action_dim, -1)\n",
    "        w2 = self.hyper_w2(state_inputs).view(batch_size, -1, 1)\n",
    "        b1 = self.hyper_b1(state_inputs).view(batch_size, self.num_agents, -1)\n",
    "        b2 = self.hyper_b2(state_inputs).view(batch_size, self.num_agents, 1)\n",
    "\n",
    "        # Mixing process\n",
    "        hidden = self.elu(torch.bmm(agent_qs, w1) + b1)\n",
    "        \n",
    "        q_total = torch.bmm(hidden, w2) + b2\n",
    "        q_total = self.hyper_w3(q_total.squeeze())\n",
    "\n",
    "        return q_total\n",
    "\n",
    "# Replay Buffer\n",
    "Transition = namedtuple(\"Transition\", (\"state\", \"actions\", \"rewards\", \"next_state\", \"dones\"))\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, *args):\n",
    "        if len(self.memory) >= self.capacity:\n",
    "            self.memory.pop(0)\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.memory), batch_size, replace=False)\n",
    "        batch = [self.memory[idx] for idx in indices]\n",
    "        return Transition(*zip(*batch))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "num_agents = 16\n",
    "np.random.seed(16)\n",
    "env = LuxAIS3GymEnv()\n",
    "env = RecordEpisode(env, save_dir=\"episodes\")\n",
    "env_params = EnvParams(map_type=1, max_steps_in_match=100)\n",
    "state_dim = 2\n",
    "action_dim = 6\n",
    "mixing_dim = 32\n",
    "N = env_params.max_steps_in_match * env_params.match_count_per_episode\n",
    "\n",
    "q_networks = [nn.Sequential(nn.Linear(state_dim, 64), nn.ReLU(), nn.Linear(64, action_dim)) for _ in range(num_agents)]\n",
    "optimizers = [optim.Adam(q.parameters(), lr=0.001) for q in q_networks]\n",
    "mixing_network = QMixNet(num_agents, state_dim, action_dim, mixing_dim)\n",
    "mixing_optimizer = optim.Adam(mixing_network.parameters(), lr=0.001)\n",
    "\n",
    "buffer = ReplayBuffer(capacity=10000)\n",
    "batch_size = 32\n",
    "epsilon = 0.1\n",
    "num_episodes = 1000\n",
    "\n",
    "def select_action(q_values, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(q_values.size(-1))\n",
    "    return [torch.argmax(q_values).item(),0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Loss: 914.7701\n",
      "Episode 1, Loss: 4320.4917\n",
      "Episode 2, Loss: 260.4322\n",
      "Episode 3, Loss: 135.6003\n",
      "Episode 4, Loss: 5157.8965\n",
      "Episode 5, Loss: 685.6636\n",
      "Episode 6, Loss: 308.9046\n",
      "Episode 7, Loss: 79.1759\n",
      "Episode 8, Loss: 12.3900\n",
      "Episode 9, Loss: 875.3238\n",
      "Episode 10, Loss: 59.8169\n",
      "Episode 11, Loss: 343.3664\n",
      "Episode 12, Loss: 101.9702\n",
      "Episode 13, Loss: 136.9545\n",
      "Episode 14, Loss: 52.1247\n",
      "Episode 15, Loss: 121.8430\n",
      "Episode 16, Loss: 364.3985\n",
      "Episode 17, Loss: 49.6874\n",
      "Episode 18, Loss: 198.9029\n",
      "Episode 19, Loss: 472.8383\n",
      "Episode 20, Loss: 191.5628\n",
      "Episode 21, Loss: 87.7426\n",
      "Episode 22, Loss: 1579.8009\n",
      "Episode 23, Loss: 435.2163\n",
      "Episode 24, Loss: 677.3134\n",
      "Episode 25, Loss: 91.5042\n",
      "Episode 26, Loss: 410.9431\n",
      "Episode 27, Loss: 82.1918\n",
      "Episode 28, Loss: 927.1182\n",
      "Episode 29, Loss: 821.4628\n",
      "Episode 30, Loss: 94.2729\n",
      "Episode 31, Loss: 94.2055\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes):\n\u001b[0;32m----> 2\u001b[0m     state, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     episode_transitions \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/24test/lib/python3.10/site-packages/luxai_s3/wrappers.py:126\u001b[0m, in \u001b[0;36mRecordEpisode.reset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset\u001b[39m(\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, seed: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, options: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    124\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[Any, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_on_reset \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 126\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_episode_and_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m     obs, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mreset(seed\u001b[38;5;241m=\u001b[39mseed, options\u001b[38;5;241m=\u001b[39moptions)\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m seed\n",
      "File \u001b[0;32m~/24test/lib/python3.10/site-packages/luxai_s3/wrappers.py:162\u001b[0m, in \u001b[0;36mRecordEpisode._save_episode_and_reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_save_episode_and_reset\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    161\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"saves to generated path based on self.save_dir and episoe id and updates relevant counters\"\"\"\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_episode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepisode_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepisode_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_id \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/24test/lib/python3.10/site-packages/luxai_s3/wrappers.py:155\u001b[0m, in \u001b[0;36mRecordEpisode.save_episode\u001b[0;34m(self, save_path)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_episode\u001b[39m(\u001b[38;5;28mself\u001b[39m, save_path: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 155\u001b[0m     episode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserialize_episode_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(save_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    157\u001b[0m         json\u001b[38;5;241m.\u001b[39mdump(episode, f)\n",
      "File \u001b[0;32m~/24test/lib/python3.10/site-packages/luxai_s3/wrappers.py:149\u001b[0m, in \u001b[0;36mRecordEpisode.serialize_episode_data\u001b[0;34m(self, episode)\u001b[0m\n\u001b[1;32m    147\u001b[0m ret[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobservations\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m serialize_env_states(episode[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstates\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactions\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m episode:\n\u001b[0;32m--> 149\u001b[0m     ret[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactions\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mserialize_env_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisode\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mactions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m ret[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m episode[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    151\u001b[0m ret[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m episode[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/24test/lib/python3.10/site-packages/luxai_s3/state.py:169\u001b[0m, in \u001b[0;36mserialize_env_actions\u001b[0;34m(env_actions)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m state \u001b[38;5;129;01min\u001b[39;00m env_actions:\n\u001b[1;32m    168\u001b[0m     state \u001b[38;5;241m=\u001b[39m flax\u001b[38;5;241m.\u001b[39mserialization\u001b[38;5;241m.\u001b[39mto_state_dict(state)\n\u001b[0;32m--> 169\u001b[0m     steps\u001b[38;5;241m.\u001b[39mappend(\u001b[43mserialize_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m steps\n",
      "File \u001b[0;32m~/24test/lib/python3.10/site-packages/luxai_s3/state.py:154\u001b[0m, in \u001b[0;36mserialize_env_actions.<locals>.serialize_array\u001b[0;34m(arr, key_path)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndarray\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for episode in range(num_episodes):\n",
    "    state, info = env.reset(seed=1, options=dict(params=env_params))\n",
    "    done = False\n",
    "    episode_transitions = []\n",
    "    i =0\n",
    "    while not done or i < N:\n",
    "        actions = np.zeros((16, 3), dtype=int)\n",
    "        q_values = {}\n",
    "        state = state['player_0'].units.position[0]\n",
    "        \n",
    "        for agent_id, q_network in zip(range(num_agents), q_networks):\n",
    "            obs = torch.tensor(np.array(state[agent_id]), dtype=torch.float32).unsqueeze(0)\n",
    "            q_values[agent_id] = q_network(obs)\n",
    "            actions[agent_id] = select_action(q_values[agent_id], epsilon)\n",
    "\n",
    "        opp_actions = env.action_space.sample()['player_1'] # random action opponents\n",
    "        act = {'player_0': actions, 'player_1': opp_actions}\n",
    "\n",
    "        next_state, rewards, terminated, truncated, info = env.step(act)\n",
    "        dones = terminated['player_0'] + truncated['player_0']\n",
    "        buffer.push(state, actions, rewards['player_0'], next_state['player_0'].units.position[0], dones)\n",
    "        state = next_state\n",
    "        done = dones\n",
    "        i = i + 1\n",
    "\n",
    "    if len(buffer) >= batch_size:\n",
    "        batch = buffer.sample(batch_size)\n",
    "        state_batch = torch.tensor(np.array([s for s in batch.state]), dtype=torch.float32)\n",
    "        action_batch = torch.tensor(np.array([a for a in batch.actions]), dtype=torch.int64)\n",
    "        reward_batch = torch.tensor(np.array([r for r in batch.rewards]), dtype=torch.float32)\n",
    "        next_state_batch = torch.tensor(np.array([ns for ns in batch.next_state]), dtype=torch.float32)\n",
    "        done_batch = torch.tensor(np.array([d for d in batch.dones]), dtype=torch.float32)\n",
    "\n",
    "        # Calculate individual Q-values\n",
    "        agent_qs = []\n",
    "        for i, q_network in enumerate(q_networks):\n",
    "            agent_qs.append(q_network(state_batch[:, i, :]))\n",
    "        agent_qs = torch.stack(agent_qs, dim=1)\n",
    "\n",
    "        # Calculate total Q-value using mixing network\n",
    "        state_inputs = state_batch.view(batch_size, -1)\n",
    "        q_total = mixing_network(agent_qs, state_inputs)\n",
    "\n",
    "        # Compute loss and update networks\n",
    "        loss = torch.mean((reward_batch - q_total) ** 2)\n",
    "\n",
    "        for optimizer in optimizers:\n",
    "            optimizer.zero_grad()\n",
    "        mixing_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        for optimizer in optimizers:\n",
    "            optimizer.step()\n",
    "        mixing_optimizer.step()\n",
    "\n",
    "    if episode % 1 == 0:\n",
    "        print(f\"Episode {episode}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'player_0': Array(0, dtype=int32), 'player_1': Array(5, dtype=int32)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "24test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
