{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2025-02-04 16:31:28,412:jax._src.xla_bridge:969: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    }
   ],
   "source": [
    "from luxai_s3.wrappers import LuxAIS3GymEnv, RecordEpisode\n",
    "from typing import Dict\n",
    "from argparse import Namespace\n",
    "import jax\n",
    "from lux.utils import direction_to\n",
    "from luxai_s3.params import EnvParams\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import namedtuple, defaultdict\n",
    "import torch.nn.functional as F\n",
    "from gen_data import gen_data\n",
    "from agent import Agent\n",
    "\n",
    "# from lux.config import EnvConfig\n",
    "from lux.kit import from_json\n",
    "env = LuxAIS3GymEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay Buffer\n",
    "Transition = namedtuple(\"Transition\", (\"state\", \"actions\", \"rewards\", \"next_state\", \"dones\"))\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, *args):\n",
    "        if len(self.memory) >= self.capacity:\n",
    "            self.memory.pop(0)\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.memory), batch_size, replace=False)\n",
    "        batch = [self.memory[idx] for idx in indices]\n",
    "        return Transition(*zip(*batch))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 에이전트 별 q-network\n",
    "class AgentQNetwork(nn.Module):\n",
    "    def __init__(self, obs_shape=(5, 24, 24), action_space=(6, 24, 24)):\n",
    "        super(AgentQNetwork, self).__init__()\n",
    "        self.action_space = action_space\n",
    "        self.conv1 = nn.Conv2d(obs_shape[0], 16, kernel_size=3, stride=1, padding=1) \n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.LazyLinear(128)\n",
    "        self.fc2 = nn.LazyLinear(action_space[0] * action_space[1] * action_space[2])\n",
    "\n",
    "    def forward(self, obs): # Input dimension -> (bs, 5, 24, 24)\n",
    "        if len(obs.shape) < 4:\n",
    "            obs = obs.unsqueeze(0) # 1개 짜리 input을 받은 경우 (1,5,24,24)로 변환\n",
    "        x = F.relu(self.conv1(obs))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = nn.Flatten()(x)  # Flatten for the fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        q_values = self.fc2(x).view((obs.shape)[0], *self.action_space) \n",
    "        return q_values # Out dimension -> (bs, 6, 24, 24)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MixerNetwork(nn.Module):\n",
    "    def __init__(self, n_agents=16, hidden_dim=64):\n",
    "        super(MixerNetwork, self).__init__()\n",
    "        self.n_agents = n_agents\n",
    "\n",
    "        # CNN으로 상태 정보 처리\n",
    "        self.state_encoder = nn.Sequential(\n",
    "            nn.Conv2d(5, 32, kernel_size=3, stride=1, padding=1),  # (5, 24, 24) -> (32, 24, 24)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),  # (32, 24, 24) -> (64, 12, 12)\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),  # (64, 12, 12) -> (64 * 12 * 12)\n",
    "        )\n",
    "        # Flatten된 state 차원 계산\n",
    "        self.flattened_state_dim = 64 * 12 * 12\n",
    "\n",
    "        # Hypernetwork for producing weights and biases\n",
    "        self.hyper_w_1 = nn.Linear(self.flattened_state_dim, n_agents * hidden_dim)\n",
    "        self.hyper_b_1 = nn.Linear(self.flattened_state_dim, hidden_dim)\n",
    "\n",
    "        self.hyper_w_2 = nn.Linear(self.flattened_state_dim, hidden_dim)\n",
    "        self.hyper_b_2 = nn.Sequential(\n",
    "            nn.Linear(self.flattened_state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "        # Non-linear transformation\n",
    "        self.elu = nn.ELU()\n",
    "\n",
    "    def forward(self, agent_qs, states):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            agent_qs (torch.Tensor): 각 에이전트의 Q-values. (batch_size, n_agents)\n",
    "            states (torch.Tensor): 전역 상태 정보. (batch_size, 5, 24, 24)\n",
    "        Returns:\n",
    "            torch.Tensor: 팀의 전체 Q-value. (batch_size, 1)\n",
    "        \"\"\"\n",
    "        batch_size = agent_qs.size(0)\n",
    "\n",
    "        # State encoding\n",
    "        encoded_state = self.state_encoder(states)  # (batch_size, flattened_state_dim)\n",
    "\n",
    "        # Hypernetwork 1: Generate weights and biases for the first layer\n",
    "        w1 = self.hyper_w_1(encoded_state)  # (batch_size, n_agents * hidden_dim)\n",
    "        w1 = w1.view(batch_size, self.n_agents, -1)  # (batch_size, n_agents, hidden_dim)\n",
    "        b1 = self.hyper_b_1(encoded_state).unsqueeze(1)  # (batch_size, 1, hidden_dim)\n",
    "\n",
    "        # First layer: Multiply agent Q-values with generated weights\n",
    "        hidden = torch.bmm(agent_qs.unsqueeze(1), w1).squeeze(1) + b1  # (batch_size, hidden_dim)\n",
    "        hidden = self.elu(hidden)\n",
    "\n",
    "        # Hypernetwork 2: Generate weights and biases for the second layer\n",
    "        w2 = self.hyper_w_2(encoded_state).unsqueeze(-1)  # (batch_size, hidden_dim, 1)\n",
    "        b2 = self.hyper_b_2(encoded_state)  # (batch_size, 1)\n",
    "\n",
    "        # Second layer: Combine hidden layer outputs\n",
    "        team_q = torch.bmm(hidden, w2).squeeze(1) + b2  # (batch_size, 1)\n",
    "\n",
    "        return team_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_action_from_qval(q_value): # sinlge agent의 batched q value(bs, 6,24,24)를 list로 받음\n",
    "    batch_size = len(q_value)\n",
    "    max_q, max_id = torch.max(q_value.view(batch_size, -1), dim =1)\n",
    "    unraveled_idx = torch.tensor([torch.unravel_index(idx, q_value.shape[1:]) for idx in max_id])\n",
    "    q_values_batched = [max_q,unraveled_idx]\n",
    "    return q_values_batched # output -> list, q_values_batched = [(bs,1), (bs,3)], 첫 째는 optimal q value, 둘 째는 해당하는 action 자체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decentralized_action(num_agent, obs, player_id, model, device, eps = 0.05): # 에이전트 별 모델을 list로 받아냄\n",
    "    num_agent = 16 \n",
    "    action = []\n",
    "    imaged_obs = torch.tensor(gen_data(obs)[player_id], dtype=torch.float, device= device)\n",
    "    with torch.no_grad():\n",
    "        for agent_id ,q_network in zip(range(num_agent), model):\n",
    "            q_network.eval()\n",
    "            q_obs = q_network(imaged_obs)\n",
    "            optimal_q = optimal_action_from_qval(q_obs)\n",
    "\n",
    "            if np.random.rand() < eps:\n",
    "                rand_act = torch.tensor([np.random.randint(6),np.random.randint(24),np.random.randint(24)])\n",
    "                rand_act = rand_act.repeat(q_obs.size(0),1)\n",
    "                action.append(rand_act)\n",
    "            else:\n",
    "                action.append(optimal_q[1])\n",
    "    return action # action을 담은 list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Envobs observation-> dict 으로 변환하는 함수\n",
    "from dataclasses import asdict\n",
    "def to_dict(obs):\n",
    "    obs_0= asdict(obs['player_0'])\n",
    "    obs_1= asdict(obs['player_1'])\n",
    "    return {'player_0': obs_0, 'player_1': obs_1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "# 초기값 및 모델 하이퍼파라미터 세팅\n",
    "num_agents = 16\n",
    "np.random.seed(16)\n",
    "env = LuxAIS3GymEnv()\n",
    "env = RecordEpisode(env, save_dir=\"episodes\")\n",
    "env_params = EnvParams(map_type=1, max_steps_in_match=100)\n",
    "N = env_params.max_steps_in_match * env_params.match_count_per_episode\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "\n",
    "q_networks = [AgentQNetwork().to(device) for _ in range(num_agents)]\n",
    "optimizers = [optim.Adam(q.parameters(), lr=0.0005) for q in q_networks]\n",
    "mixing_network = MixerNetwork(hidden_dim=64).to(device)\n",
    "mixing_optimizer = optim.Adam(mixing_network.parameters(), lr=0.001)\n",
    "\n",
    "buffer = ReplayBuffer(capacity=5000)\n",
    "batch_size = 128\n",
    "epsilon = 0.05\n",
    "num_episodes = 125\n",
    "GAMMA = 0.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Loss: 57.0249, my_team_points: [0, 0, 13, 0, 0],  my_team_wins: 0\n",
      "Episode 2, Loss: 3818.2107, my_team_points: [35, 60, 38, 45, 20],  my_team_wins: 0\n",
      "Episode 3, Loss: 8082.2231, my_team_points: [0, 3, 0, 0, 8],  my_team_wins: 0\n",
      "Episode 4, Loss: 6447.7573, my_team_points: [58, 21, 55, 44, 26],  my_team_wins: 0\n",
      "Episode 5, Loss: 1432.2297, my_team_points: [8, 0, 0, 0, 2],  my_team_wins: 0\n",
      "Episode 6, Loss: 903.3110, my_team_points: [7, 0, 0, 2, 0],  my_team_wins: 0\n",
      "Episode 7, Loss: 2583.2397, my_team_points: [32, 62, 35, 9, 27],  my_team_wins: 0\n",
      "Episode 8, Loss: 498.6404, my_team_points: [0, 4, 1, 0, 0],  my_team_wins: 0\n",
      "Episode 9, Loss: 670.8605, my_team_points: [43, 58, 45, 48, 31],  my_team_wins: 0\n",
      "Episode 10, Loss: 480.0352, my_team_points: [0, 0, 0, 0, 4],  my_team_wins: 0\n",
      "Episode 11, Loss: 373.6354, my_team_points: [55, 42, 69, 31, 17],  my_team_wins: 0\n",
      "Episode 12, Loss: 465.4753, my_team_points: [0, 0, 39, 21, 5],  my_team_wins: 0\n",
      "Episode 13, Loss: 447.3589, my_team_points: [0, 0, 0, 2, 0],  my_team_wins: 0\n",
      "Episode 14, Loss: 299.1530, my_team_points: [61, 50, 33, 20, 17],  my_team_wins: 0\n",
      "Episode 15, Loss: 297.4907, my_team_points: [2, 16, 0, 0, 0],  my_team_wins: 1\n",
      "Episode 16, Loss: 374.6001, my_team_points: [14, 44, 11, 14, 15],  my_team_wins: 0\n",
      "Episode 17, Loss: 373.5506, my_team_points: [56, 23, 9, 46, 84],  my_team_wins: 0\n",
      "Episode 18, Loss: 402.4804, my_team_points: [0, 4, 18, 37, 14],  my_team_wins: 0\n",
      "Episode 19, Loss: 198.1609, my_team_points: [18, 0, 0, 11, 3],  my_team_wins: 0\n",
      "Episode 20, Loss: 247.8382, my_team_points: [18, 9, 8, 10, 13],  my_team_wins: 1\n",
      "Episode 21, Loss: 260.3317, my_team_points: [0, 6, 1, 0, 0],  my_team_wins: 0\n",
      "Episode 22, Loss: 240.7383, my_team_points: [7, 12, 0, 18, 2],  my_team_wins: 0\n",
      "Episode 23, Loss: 158.7141, my_team_points: [0, 0, 13, 0, 0],  my_team_wins: 0\n",
      "Episode 24, Loss: 200.2313, my_team_points: [0, 11, 0, 1, 0],  my_team_wins: 2\n",
      "Episode 25, Loss: 187.4812, my_team_points: [15, 0, 11, 19, 27],  my_team_wins: 0\n",
      "Episode 26, Loss: 182.5749, my_team_points: [11, 9, 2, 15, 25],  my_team_wins: 1\n",
      "Episode 27, Loss: 248.3257, my_team_points: [17, 23, 11, 62, 5],  my_team_wins: 0\n",
      "Episode 28, Loss: 119.8452, my_team_points: [3, 0, 1, 0, 4],  my_team_wins: 0\n",
      "Episode 29, Loss: 126.9189, my_team_points: [16, 1, 61, 12, 1],  my_team_wins: 0\n",
      "Episode 30, Loss: 179.6561, my_team_points: [26, 1, 51, 0, 1],  my_team_wins: 0\n",
      "Episode 31, Loss: 192.1386, my_team_points: [3, 14, 0, 0, 2],  my_team_wins: 1\n",
      "Episode 32, Loss: 93.0379, my_team_points: [16, 16, 20, 43, 14],  my_team_wins: 0\n",
      "Episode 33, Loss: 220.2007, my_team_points: [17, 0, 2, 0, 6],  my_team_wins: 0\n",
      "Episode 34, Loss: 202.9796, my_team_points: [6, 8, 5, 14, 3],  my_team_wins: 1\n",
      "Episode 35, Loss: 88.3562, my_team_points: [0, 0, 2, 8, 0],  my_team_wins: 1\n",
      "Episode 36, Loss: 216.3367, my_team_points: [45, 15, 50, 65, 8],  my_team_wins: 0\n",
      "Episode 37, Loss: 210.0396, my_team_points: [4, 5, 3, 0, 9],  my_team_wins: 0\n",
      "Episode 38, Loss: 222.9581, my_team_points: [2, 6, 9, 3, 1],  my_team_wins: 0\n",
      "Episode 39, Loss: 138.0156, my_team_points: [58, 18, 37, 14, 17],  my_team_wins: 0\n",
      "Episode 40, Loss: 710.0440, my_team_points: [94, 49, 99, 94, 75],  my_team_wins: 3\n",
      "Episode 41, Loss: 805.4928, my_team_points: [19, 40, 48, 55, 68],  my_team_wins: 0\n",
      "Episode 42, Loss: 443.8628, my_team_points: [46, 89, 73, 74, 118],  my_team_wins: 0\n",
      "Episode 43, Loss: 1103.7024, my_team_points: [0, 0, 0, 0, 4],  my_team_wins: 0\n",
      "Episode 44, Loss: 759.9601, my_team_points: [0, 0, 18, 23, 5],  my_team_wins: 0\n",
      "Episode 45, Loss: 1235.1172, my_team_points: [0, 3, 0, 4, 0],  my_team_wins: 1\n",
      "Episode 46, Loss: 636.0637, my_team_points: [29, 15, 38, 23, 52],  my_team_wins: 0\n",
      "Episode 47, Loss: 779.2320, my_team_points: [0, 32, 3, 0, 0],  my_team_wins: 0\n",
      "Episode 48, Loss: 2167.5566, my_team_points: [219, 133, 169, 211, 195],  my_team_wins: 0\n",
      "Episode 49, Loss: 2215.4944, my_team_points: [3, 4, 4, 0, 0],  my_team_wins: 0\n",
      "Episode 50, Loss: 1289.8904, my_team_points: [0, 6, 0, 12, 0],  my_team_wins: 4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m     opp_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(decentralized_action(\u001b[38;5;241m16\u001b[39m, states, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplayer_1\u001b[39m\u001b[38;5;124m'\u001b[39m,  q_networks, device, epsilon))\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m16\u001b[39m,\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 26\u001b[0m     opp_actions \u001b[38;5;241m=\u001b[39m \u001b[43mopp_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#opp_actions = env.action_space.sample()['player_1'] # random action opponents로 학습도 가능\u001b[39;00m\n\u001b[1;32m     29\u001b[0m act \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplayer_0\u001b[39m\u001b[38;5;124m'\u001b[39m: actions, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplayer_1\u001b[39m\u001b[38;5;124m'\u001b[39m: opp_actions}\n",
      "File \u001b[0;32m~/lux-ai-season-3/agent.py:25\u001b[0m, in \u001b[0;36mAgent.act\u001b[0;34m(self, step, obs, remainingOverageTime)\u001b[0m\n\u001b[1;32m     23\u001b[0m unit_positions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(obs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplayer][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munits\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposition\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mteam_id]) \u001b[38;5;66;03m# shape (max_units, 2)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m unit_energys \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(obs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplayer][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munits\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menergy\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mteam_id]) \u001b[38;5;66;03m# shape (max_units, 1)\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m observed_relic_node_positions \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplayer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrelic_nodes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# shape (max_relic_nodes, 2)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m observed_relic_nodes_mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(obs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplayer][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelic_nodes_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;66;03m# shape (max_relic_nodes, )\u001b[39;00m\n\u001b[1;32m     27\u001b[0m team_points \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(obs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplayer][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mteam_points\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;66;03m# points of each team, team_points[self.team_id] is the points of the your team\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training process\n",
    "# 해당 셀을 실행하면 학습이 진행됩니다.\n",
    "import time\n",
    "self_play = False\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    states, info = env.reset(seed=np.random.seed(16), options=dict(params=env_params))\n",
    "    done = False\n",
    "    episode_transitions = []\n",
    "    i = 0\n",
    "    step = 0\n",
    "    prev_reward = 0\n",
    "    sparse_prev_reward = 0\n",
    "    team_pt = []\n",
    "    opp_agent = Agent('player_1', info['params'])\n",
    "\n",
    "    while not done and i < N:\n",
    "\n",
    "        i += 1\n",
    "        actions = np.array(decentralized_action(16, states, 'player_0',  q_networks, device, epsilon)).reshape(16,3)\n",
    "\n",
    "        if self_play:\n",
    "        # self-play 기반 opponent action 생성\n",
    "            opp_actions = np.array(decentralized_action(16, states, 'player_1',  q_networks, device, epsilon)).reshape(16,3)\n",
    "        else:\n",
    "            opp_actions = opp_agent.act(step =step , obs= to_dict(states))\n",
    "                \n",
    "        #opp_actions = env.action_space.sample()['player_1'] # random action opponents로 학습도 가능\n",
    "        act = {'player_0': actions, 'player_1': opp_actions}\n",
    "\n",
    "        next_state, rewards, terminated, truncated, info = env.step(act)\n",
    "        dones = terminated['player_0'] + truncated['player_0']\n",
    "\n",
    "        # Rewards engineering\n",
    "        dense_rewards = next_state['player_0'].team_points[0] - prev_reward \n",
    "        sparse_rewards = (rewards['player_0'] - sparse_prev_reward) * 100\n",
    "\n",
    "        buffer.push(gen_data(states)['player_0'], actions, dense_rewards, gen_data(next_state)['player_0'], dones) # 버퍼에는 내 플레이어의 정보들만 저장\n",
    "\n",
    "        states = next_state\n",
    "        done = dones\n",
    "        prev_reward = dense_rewards\n",
    "        sparse_prev_reward = sparse_rewards\n",
    "\n",
    "        step +=1\n",
    "\n",
    "        # reward reset per match\n",
    "        if done or i % 100 == 0:\n",
    "            step =0\n",
    "            prev_reward = 0\n",
    "            sparse_prev_reward = 0\n",
    "            team_pt.append(states['player_0'].team_points[0].item())\n",
    "\n",
    "    # optimizing 하는 루프\n",
    "    if len(buffer) >= batch_size:\n",
    "        batch = buffer.sample(batch_size)\n",
    "        state_batch = torch.tensor(np.array([s for s in batch.state]), dtype=torch.float32, device=device)\n",
    "        action_batch = torch.tensor(np.array([a for a in batch.actions]), dtype=torch.int64, device=device)\n",
    "        reward_batch = torch.tensor(np.array([r for r in batch.rewards]), dtype=torch.float32, device=device)\n",
    "        next_state_batch = torch.tensor(np.array([ns for ns in batch.next_state]), dtype=torch.float32, device=device)\n",
    "        done_batch = torch.tensor(np.array([d for d in batch.dones]), dtype=torch.float32, device=device)\n",
    "\n",
    "    # Calculate individual Q-values\n",
    "        agent_qs = []; agent_qs_next = []\n",
    "        for i, q_network in enumerate(q_networks):\n",
    "            q_value_single = q_network(state_batch)\n",
    "            action_single = action_batch[:,i,:]\n",
    "            opt_q_value = torch.tensor([q_value_single[tuple(torch.hstack([torch.tensor(i),idx]).tolist())] for i, idx in zip(range(batch_size),action_single)])\n",
    "            agent_qs.append(opt_q_value)\n",
    "\n",
    "            next_q_value_single = q_network(next_state_batch)\n",
    "            opt_next_q_value = optimal_action_from_qval(next_q_value_single)[0]\n",
    "            agent_qs_next.append(opt_next_q_value)\n",
    "\n",
    "        agent_qs = torch.stack(agent_qs, dim=1)\n",
    "        agent_qs_next = torch.stack(agent_qs_next, dim=1)\n",
    "\n",
    "        # Calculate total Q-value using mixing network\n",
    "        state_inputs = state_batch.view(batch_size, 5,24,24)\n",
    "        q_total = mixing_network(agent_qs, state_inputs)\n",
    "\n",
    "        # Calculate total next Q-value using mixing network\n",
    "        next_state_inputs = next_state_batch.view(batch_size, 5,24,24)\n",
    "        with torch.no_grad():\n",
    "            q_total_next = mixing_network(agent_qs_next, next_state_inputs)\n",
    "\n",
    "        # Compute loss and update networks\n",
    "        loss = torch.mean((reward_batch + GAMMA*q_total_next - q_total) ** 2)\n",
    "\n",
    "        for optimizer in optimizers:\n",
    "            optimizer.zero_grad()\n",
    "        mixing_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        for optimizer in optimizers:\n",
    "            optimizer.step()\n",
    "        mixing_optimizer.step()\n",
    "\n",
    "    if episode % 1 == 0:\n",
    "        print(f\"Episode {episode+1}, Loss: {loss.item():.4f}, my_team_points: {team_pt},  my_team_wins: {rewards['player_0']}\")\n",
    "\n",
    "    if episode % 15 == 0:\n",
    "        q_net = nn.ModuleList(q_networks)\n",
    "        torch.save(q_net, \"q_net.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수동 저장\n",
    "# 체크포인트 생성 및 모델의 자동 업데이트 기능은 아직 없습니다\n",
    "q_net = nn.ModuleList(q_networks)\n",
    "torch.save(q_net, \"q_net.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "24test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
